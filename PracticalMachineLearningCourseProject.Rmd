---
title: "Practical Machine Learning Course Project"
author: "Frederica Janga"
date: "1 februari 2017"
output: html_document
---

## Background and Introduction

Nowadays it is possible for several apps like Nike, Fitbit and Runkeeper, to track a lot of data about personal activities. These apps are a part of a group of investigators who take measurements about themselves regularly to improve their health or to find patterns in their behavior. One thing that people measure most of the time is how much time they are doing a particular exercise. But they rarely quantify how well the exercise is done.

In this paper, data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants is analyzed. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The goal of this paper is to predict the manner in which the participants did the exercise. The "classe" variable in the training set is used to measure this. 
In the first part, the model is built and used with cross validation. 
Then the expected out of sample error is calculated.
In the end, the prediction model is used to predict 20 different test cases.

## Exploratory data analysis

### Read in the data

####Read in the needed packages.

```{r, echo = FALSE, message=FALSE, results = 'hide'}
#Read in the needed packages.
library(caret); library(randomForest); library(rpart);library(rattle)
library(rpart.plot)
```

```{r, echo=TRUE}
#Read in the data and clean up the data.
training <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA", ""))
dim(training); dim(testing)

#change NA values into zero
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]

#Delete the first 7 columns
training <- training[, -c(1:7)]
testing <- testing[, -c(1:7)]
dim(training); dim(testing)
```

As you can see after cleaning up the data, the training set consists of 19622 rows and 53 columns. And the testing set consists of 20 rows and 53 columns.

### Data splitting

Split the training data into a training and a testing set. 
(60% trainset, 40% testset) 

```{r, echo = TRUE}
set.seed(1234)
#set the training set 60% and the testing set 40%.
inTrain <- createDataPartition(y=training$classe , p=0.6, list=FALSE)
trainset <- training[inTrain,]
testset <- training[-inTrain,]   
```

## Model building and out of sample error

### R part method and decision tree

```{r, echo=TRUE}
#train the data with the rpart method.
modrpart <- train(classe ~., method="rpart", data=trainset)

print(modrpart$finalModel)

#Make a classification tree plot with the rpart method.
fancyRpartPlot(modrpart$finalModel)

#Calculate the predictions
predictionsrpart <- predict(modrpart, newdata=testset)

#Set up the confusion matrix to check the accuracy.
confusionMatrix(predictionsrpart, testset$classe)
```

The accuracy is 0.49. This means the out of sample error is 0.51. From this error we can conclude that the classification tree is not the best choice to predict the classe.
Let's take a look at the Random Forest Method.

### Random forests

Now use the random forests method to train the data. We expect the random forest method will be a better predicter and will give a higher accuracy.

```{r, echo=TRUE}
#Use the randomForest method
modforest <- randomForest(classe ~.,data = trainset)

print(modforest)

#Calculate the predictions from the random forest method.
predictionsforest <- predict(modforest, newdata=testset)

#Set up the confusion matrix
confusionMatrix(predictionsforest, testset$classe)

```

The accuracy is 0.9913. This means the out of sample error is 0.0087. From this error we can conclude that the random forest method is way better a good predictor then the rpart method. 

## Prediction case

Let's use the random forest method to predict 20 different test cases.

```{r, echo=FALSE, message = FALSE}
prediction <- predict(modforest, testing, type="class")
prediction
```

## Conclusion
After using the rpart method and the random forest method, the random forest has by far the highest accuracy and is the best model to use for prediction.